{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import gymGharrafa\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time\n",
    "from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from baselines.common import cmd_util\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from collections import deque  \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience:\n",
    "    def __init__(self, subEnvs, nsteps, gamma, policy, critic, possible_actions, device):\n",
    "        self.policy = policy\n",
    "        self.critic = critic\n",
    "        self.subEnvs = subEnvs\n",
    "        self.steps = nsteps\n",
    "        self.state = subEnvs.reset()\n",
    "        self.last_steps = deque([])\n",
    "        self.reward_accumulator = 0\n",
    "        self.episodes_count = 0\n",
    "        self.possible_actions = possible_actions\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        \n",
    "        #for monitoring\n",
    "        self.currentscores = np.zeros(subEnvs.num_envs)\n",
    "        self.accscores = np.zeros(subEnvs.num_envs)\n",
    "        self.finished = np.zeros(subEnvs.num_envs)\n",
    "        self.lastfinished = np.zeros(subEnvs.num_envs)\n",
    "        self.lastaccscores = np.zeros(subEnvs.num_envs)\n",
    "        \n",
    "        self.probs = []\n",
    "    \n",
    "    def getStats(self):\n",
    "        entropy = np.mean([np.mean([stats.entropy(a[pi]) for pi in range(a.shape[0])]) for a in self.probs[-100:]])\n",
    "        \n",
    "        if 0 not in self.finished: \n",
    "            mean_return = np.mean(self.accscores / self.finished)\n",
    "\n",
    "        else:\n",
    "            mean_return = 0\n",
    "        self.probs = self.probs[-100:]\n",
    "        gc.collect()\n",
    "        return mean_return,entropy,self.episodes_count\n",
    "    \n",
    "    def update(self,policy,critic):\n",
    "        self.policy = policy\n",
    "        self.critic = critic\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def __next__(self):\n",
    "        for i in range((self.steps+1)-len(self.last_steps)): \n",
    "            if self.policy:\n",
    "                obsTensor = torch.FloatTensor(self.state.reshape(self.subEnvs.num_envs,subEnvs.observation_space.shape[1])).to(self.device)\n",
    "                softmax = F.softmax(self.policy(obsTensor))\n",
    "                probs = softmax.detach().cpu().numpy() \n",
    "                self.probs.append(probs)\n",
    "                actions = np.array([ np.random.choice(self.possible_actions,p=probs[c]) for c in range(self.subEnvs.num_envs)] )\n",
    "            else:\n",
    "                actions = np.array([random.choice(self.possible_actions) for _ in range(self.subEnvs.num_envs)])\n",
    "            newstate,reward,episode_over,info = self.subEnvs.step(actions)\n",
    "\n",
    "            #print(episode_over)\n",
    "            #print(\"Time: %2f. Reward: %2f. Episode over: %s\" % (info['time'],reward,\"YES\" if episode_over else \"NO\"))\n",
    "            \n",
    "            #add final score of finished episodes\n",
    "            self.currentscores += reward\n",
    "            self.accscores += self.currentscores * episode_over\n",
    "            self.finished += episode_over\n",
    "            \n",
    "            #reset currentscores for finished episodes\n",
    "            self.currentscores  = self.currentscores * ~episode_over\n",
    "            \n",
    "            self.reward_accumulator += np.sum(reward)\n",
    "            self.episodes_count += np.sum(episode_over)\n",
    "            \n",
    "            \n",
    "            self.last_steps.append((self.state,reward,episode_over,actions))\n",
    "            self.state = newstate\n",
    "\n",
    "        first_state,accumulated_reward,was_end_reached,first_actions = self.last_steps.popleft()\n",
    "        \n",
    "        gammap=1.0\n",
    "        for i in range(self.steps-1):\n",
    "            gammap = gammap*self.gamma\n",
    "            _,reward,end_reached,_ = self.last_steps[i]\n",
    "            \n",
    "            accumulated_reward += reward*gammap* ~was_end_reached\n",
    "            \n",
    "            was_end_reached += end_reached\n",
    "\n",
    "        last_state = self.last_steps[-1][0]*(~was_end_reached)[:,None]\n",
    "\n",
    "        \n",
    "        return (first_state,accumulated_reward,last_state,first_actions,was_end_reached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.00001\n",
    "ENTROPY_BETA = 0.001\n",
    "BATCH_SIZE = 32\n",
    "NUM_ENVS = 6\n",
    "nsteps = 20\n",
    "\n",
    "CLIP_GRAD = 0.2\n",
    "\n",
    "possible_actions = range(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetPG(nn.Module):\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super(NetPG, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /tmp/openai-2019-04-01-11-03-49-335466\n"
     ]
    }
   ],
   "source": [
    "subEnvs = cmd_util.make_vec_env(\"gymGharrafa-v1\",gymGharrafa.GharrafaBasicEnv,NUM_ENVS,1232)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = None\n",
    "critic = None\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "exp = Experience(subEnvs,nsteps, GAMMA, policy, critic,possible_actions, device)\n",
    "\n",
    "writer = SummaryWriter(comment=\"ANN-Roundabout_NOCRITIC. Nenv %d,batch length %d, nsteps %d, eta %2f\" % (NUM_ENVS,BATCH_SIZE,nsteps,LEARNING_RATE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean entropy of last action: nan. Average score after 1 batches and 0 episodes: 0.000000\n",
      "Mean entropy of last action: 2.046667. Average score after 2 batches and 2 episodes: 0.000000\n",
      "Mean entropy of last action: 2.068501. Average score after 3 batches and 3 episodes: 0.000000\n",
      "Mean entropy of last action: 2.046592. Average score after 4 batches and 4 episodes: 0.000000\n",
      "Mean entropy of last action: 2.046337. Average score after 5 batches and 6 episodes: 0.000000\n",
      "Mean entropy of last action: 2.050170. Average score after 6 batches and 7 episodes: 665.833333\n",
      "Mean entropy of last action: 2.052599. Average score after 7 batches and 10 episodes: 655.305556\n",
      "Mean entropy of last action: 2.045501. Average score after 8 batches and 10 episodes: 655.305556\n",
      "Mean entropy of last action: 2.022125. Average score after 9 batches and 11 episodes: 649.722222\n",
      "Mean entropy of last action: 2.012846. Average score after 10 batches and 12 episodes: 618.138889\n",
      "Mean entropy of last action: 2.002524. Average score after 11 batches and 14 episodes: 746.222222\n",
      "Mean entropy of last action: 1.983945. Average score after 12 batches and 15 episodes: 709.861111\n",
      "Mean entropy of last action: 1.997825. Average score after 13 batches and 17 episodes: 739.166667\n",
      "Mean entropy of last action: 2.024198. Average score after 14 batches and 19 episodes: 752.222222\n",
      "Mean entropy of last action: 2.055875. Average score after 15 batches and 20 episodes: 754.388889\n",
      "Mean entropy of last action: 2.045530. Average score after 16 batches and 20 episodes: 754.388889\n",
      "Mean entropy of last action: 2.024467. Average score after 17 batches and 21 episodes: 733.569444\n",
      "Mean entropy of last action: 2.016017. Average score after 18 batches and 23 episodes: 749.930556\n",
      "Mean entropy of last action: 2.059944. Average score after 19 batches and 25 episodes: 768.708333\n",
      "Mean entropy of last action: 2.063882. Average score after 20 batches and 25 episodes: 768.708333\n",
      "Mean entropy of last action: 2.016923. Average score after 21 batches and 27 episodes: 782.786111\n",
      "Mean entropy of last action: 1.983198. Average score after 22 batches and 28 episodes: 788.461111\n",
      "Mean entropy of last action: 1.989012. Average score after 23 batches and 31 episodes: 776.092857\n",
      "Mean entropy of last action: 2.106936. Average score after 24 batches and 32 episodes: 788.576190\n",
      "Mean entropy of last action: 2.138689. Average score after 25 batches and 32 episodes: 788.576190\n",
      "Mean entropy of last action: 2.164940. Average score after 26 batches and 35 episodes: 771.897222\n",
      "Mean entropy of last action: 2.141404. Average score after 27 batches and 36 episodes: 779.847222\n",
      "Mean entropy of last action: 2.128981. Average score after 28 batches and 38 episodes: 783.603704\n",
      "Mean entropy of last action: 2.119419. Average score after 29 batches and 39 episodes: 773.131481\n",
      "Mean entropy of last action: 2.092983. Average score after 30 batches and 39 episodes: 773.131481\n",
      "Mean entropy of last action: 2.038978. Average score after 31 batches and 39 episodes: 773.131481\n",
      "Mean entropy of last action: 1.977168. Average score after 32 batches and 39 episodes: 773.131481\n",
      "Mean entropy of last action: 1.979800. Average score after 33 batches and 42 episodes: 785.004497\n",
      "Mean entropy of last action: 2.014745. Average score after 34 batches and 43 episodes: 805.587831\n",
      "Mean entropy of last action: 2.047829. Average score after 35 batches and 45 episodes: 839.636243\n",
      "Mean entropy of last action: 2.068951. Average score after 36 batches and 46 episodes: 835.506614\n",
      "Mean entropy of last action: 2.052466. Average score after 37 batches and 48 episodes: 856.425926\n",
      "Mean entropy of last action: 2.070148. Average score after 38 batches and 49 episodes: 838.382275\n",
      "Mean entropy of last action: 2.025626. Average score after 39 batches and 49 episodes: 838.382275\n",
      "Mean entropy of last action: 1.997204. Average score after 40 batches and 50 episodes: 829.697090\n",
      "Mean entropy of last action: 2.001955. Average score after 41 batches and 52 episodes: 845.497553\n",
      "Mean entropy of last action: 2.045070. Average score after 42 batches and 53 episodes: 854.941997\n",
      "Mean entropy of last action: 2.096747. Average score after 43 batches and 54 episodes: 861.676846\n",
      "Mean entropy of last action: 2.081789. Average score after 44 batches and 56 episodes: 860.774663\n",
      "Mean entropy of last action: 2.063626. Average score after 45 batches and 56 episodes: 860.774663\n",
      "Mean entropy of last action: 2.055866. Average score after 46 batches and 58 episodes: 866.411364\n",
      "Mean entropy of last action: 2.072721. Average score after 47 batches and 59 episodes: 852.575715\n",
      "Mean entropy of last action: 2.035694. Average score after 48 batches and 60 episodes: 860.647475\n",
      "Mean entropy of last action: 2.014987. Average score after 49 batches and 62 episodes: 851.325926\n",
      "Mean entropy of last action: 2.002824. Average score after 50 batches and 62 episodes: 851.325926\n",
      "Mean entropy of last action: 2.011079. Average score after 51 batches and 63 episodes: 864.063300\n",
      "Mean entropy of last action: 2.011665. Average score after 52 batches and 65 episodes: 861.479461\n",
      "Mean entropy of last action: 2.009446. Average score after 53 batches and 65 episodes: 861.479461\n",
      "Mean entropy of last action: 1.989063. Average score after 54 batches and 66 episodes: 876.105219\n",
      "Mean entropy of last action: 2.015262. Average score after 55 batches and 68 episodes: 884.040657\n",
      "Mean entropy of last action: 2.063345. Average score after 56 batches and 70 episodes: 883.641084\n",
      "Mean entropy of last action: 2.085897. Average score after 57 batches and 70 episodes: 883.641084\n",
      "Mean entropy of last action: 2.057422. Average score after 58 batches and 71 episodes: 879.579215\n",
      "Mean entropy of last action: 2.006318. Average score after 59 batches and 73 episodes: 876.335567\n",
      "Mean entropy of last action: 2.018378. Average score after 60 batches and 73 episodes: 876.335567\n",
      "Mean entropy of last action: 2.079129. Average score after 61 batches and 77 episodes: 886.239746\n",
      "Mean entropy of last action: 2.147832. Average score after 62 batches and 78 episodes: 881.553849\n",
      "Mean entropy of last action: 2.148467. Average score after 63 batches and 79 episodes: 875.285534\n",
      "Mean entropy of last action: 2.160913. Average score after 64 batches and 82 episodes: 862.098080\n",
      "Mean entropy of last action: 2.124579. Average score after 65 batches and 83 episodes: 865.094292\n",
      "Mean entropy of last action: 2.118797. Average score after 66 batches and 84 episodes: 860.061752\n",
      "Mean entropy of last action: 2.094802. Average score after 67 batches and 85 episodes: 856.488492\n",
      "Mean entropy of last action: 2.076997. Average score after 68 batches and 88 episodes: 863.405159\n",
      "Mean entropy of last action: 2.095059. Average score after 69 batches and 89 episodes: 861.116270\n",
      "Mean entropy of last action: 2.146697. Average score after 70 batches and 91 episodes: 854.886325\n",
      "Mean entropy of last action: 2.162063. Average score after 71 batches and 92 episodes: 851.087305\n",
      "Mean entropy of last action: 2.158111. Average score after 72 batches and 93 episodes: 848.356913\n",
      "Mean entropy of last action: 2.098683. Average score after 73 batches and 93 episodes: 848.356913\n",
      "Mean entropy of last action: 2.061614. Average score after 74 batches and 95 episodes: 848.646796\n",
      "Mean entropy of last action: 2.063324. Average score after 75 batches and 97 episodes: 849.185946\n",
      "Mean entropy of last action: 2.088158. Average score after 76 batches and 98 episodes: 853.840112\n",
      "Mean entropy of last action: 2.105564. Average score after 77 batches and 99 episodes: 851.372764\n",
      "Mean entropy of last action: 2.085466. Average score after 78 batches and 101 episodes: 860.271661\n",
      "Mean entropy of last action: 2.075117. Average score after 79 batches and 101 episodes: 860.271661\n",
      "Mean entropy of last action: 2.064736. Average score after 80 batches and 102 episodes: 860.063724\n",
      "Mean entropy of last action: 2.033058. Average score after 81 batches and 103 episodes: 864.760118\n",
      "Mean entropy of last action: 2.055021. Average score after 82 batches and 107 episodes: 864.990294\n",
      "Mean entropy of last action: 2.102746. Average score after 83 batches and 107 episodes: 864.990294\n",
      "Mean entropy of last action: 2.153152. Average score after 84 batches and 109 episodes: 857.418303\n",
      "Mean entropy of last action: 2.166008. Average score after 85 batches and 110 episodes: 856.074652\n",
      "Mean entropy of last action: 2.158553. Average score after 86 batches and 111 episodes: 854.195567\n",
      "Mean entropy of last action: 2.187282. Average score after 87 batches and 114 episodes: 846.819527\n",
      "Mean entropy of last action: 2.159621. Average score after 88 batches and 114 episodes: 846.819527\n",
      "Mean entropy of last action: 2.108792. Average score after 89 batches and 115 episodes: 846.569527\n",
      "Mean entropy of last action: 2.082952. Average score after 90 batches and 118 episodes: 852.902109\n",
      "Mean entropy of last action: 2.092710. Average score after 91 batches and 118 episodes: 852.902109\n",
      "Mean entropy of last action: 2.122590. Average score after 92 batches and 119 episodes: 853.124916\n",
      "Mean entropy of last action: 2.150920. Average score after 93 batches and 122 episodes: 854.670853\n",
      "Mean entropy of last action: 2.175762. Average score after 94 batches and 122 episodes: 854.670853\n",
      "Mean entropy of last action: 2.199046. Average score after 95 batches and 123 episodes: 853.396974\n",
      "Mean entropy of last action: 2.165948. Average score after 96 batches and 123 episodes: 853.396974\n",
      "Mean entropy of last action: 2.117861. Average score after 97 batches and 125 episodes: 857.188158\n",
      "Mean entropy of last action: 2.062320. Average score after 98 batches and 125 episodes: 857.188158\n",
      "Mean entropy of last action: 2.097397. Average score after 99 batches and 128 episodes: 865.139384\n",
      "Mean entropy of last action: 2.118745. Average score after 100 batches and 128 episodes: 865.139384\n",
      "Mean entropy of last action: 2.111809. Average score after 101 batches and 128 episodes: 865.139384\n",
      "Mean entropy of last action: 2.080085. Average score after 102 batches and 129 episodes: 866.473115\n",
      "Mean entropy of last action: 2.053545. Average score after 103 batches and 129 episodes: 866.473115\n",
      "Mean entropy of last action: 2.060026. Average score after 104 batches and 130 episodes: 871.015337\n",
      "Mean entropy of last action: 2.092724. Average score after 105 batches and 132 episodes: 875.618041\n",
      "Mean entropy of last action: 2.117276. Average score after 106 batches and 133 episodes: 888.529883\n",
      "Mean entropy of last action: 2.133315. Average score after 107 batches and 134 episodes: 895.805575\n",
      "Mean entropy of last action: 2.137497. Average score after 108 batches and 135 episodes: 896.395831\n",
      "Mean entropy of last action: 2.137305. Average score after 109 batches and 136 episodes: 897.143416\n",
      "Mean entropy of last action: 2.125759. Average score after 110 batches and 137 episodes: 894.173911\n",
      "Mean entropy of last action: 2.106030. Average score after 111 batches and 139 episodes: 905.301083\n",
      "Mean entropy of last action: 2.095290. Average score after 112 batches and 139 episodes: 905.301083\n",
      "Mean entropy of last action: 2.110441. Average score after 113 batches and 140 episodes: 908.649893\n",
      "Mean entropy of last action: 2.128030. Average score after 114 batches and 141 episodes: 907.891004\n",
      "Mean entropy of last action: 2.125830. Average score after 115 batches and 142 episodes: 909.285726\n",
      "Mean entropy of last action: 2.126323. Average score after 116 batches and 142 episodes: 909.285726\n",
      "Mean entropy of last action: 2.121166. Average score after 117 batches and 142 episodes: 909.285726\n",
      "Mean entropy of last action: 2.124262. Average score after 118 batches and 143 episodes: 916.801396\n",
      "Mean entropy of last action: 2.138839. Average score after 119 batches and 144 episodes: 917.686677\n",
      "Mean entropy of last action: 2.131779. Average score after 120 batches and 146 episodes: 921.369754\n",
      "Mean entropy of last action: 2.129163. Average score after 121 batches and 146 episodes: 921.369754\n",
      "Mean entropy of last action: 2.104088. Average score after 122 batches and 149 episodes: 943.502889\n",
      "Mean entropy of last action: 2.126634. Average score after 123 batches and 149 episodes: 943.502889\n",
      "Mean entropy of last action: 2.134022. Average score after 124 batches and 149 episodes: 943.502889\n",
      "Mean entropy of last action: 2.120476. Average score after 125 batches and 150 episodes: 944.411246\n",
      "Mean entropy of last action: 2.087251. Average score after 126 batches and 151 episodes: 945.037272\n",
      "Mean entropy of last action: 2.072875. Average score after 127 batches and 152 episodes: 945.030050\n",
      "Mean entropy of last action: 2.109853. Average score after 128 batches and 153 episodes: 949.278863\n",
      "Mean entropy of last action: 2.114412. Average score after 129 batches and 153 episodes: 949.278863\n",
      "Mean entropy of last action: 2.128300. Average score after 130 batches and 155 episodes: 956.506918\n",
      "Mean entropy of last action: 2.117505. Average score after 131 batches and 155 episodes: 956.506918\n",
      "Mean entropy of last action: 2.106364. Average score after 132 batches and 155 episodes: 956.506918\n",
      "Mean entropy of last action: 2.093726. Average score after 133 batches and 156 episodes: 959.852320\n",
      "Mean entropy of last action: 2.098238. Average score after 134 batches and 157 episodes: 969.346282\n",
      "Mean entropy of last action: 2.109669. Average score after 135 batches and 158 episodes: 969.217961\n",
      "Mean entropy of last action: 2.112407. Average score after 136 batches and 159 episodes: 978.847149\n",
      "Mean entropy of last action: 2.110012. Average score after 137 batches and 160 episodes: 980.566997\n",
      "Mean entropy of last action: 2.112077. Average score after 138 batches and 160 episodes: 980.566997\n",
      "Mean entropy of last action: 2.120417. Average score after 139 batches and 163 episodes: 993.638673\n",
      "Mean entropy of last action: 2.136156. Average score after 140 batches and 163 episodes: 993.638673\n",
      "Mean entropy of last action: 2.133798. Average score after 141 batches and 163 episodes: 993.638673\n",
      "Mean entropy of last action: 2.107032. Average score after 142 batches and 164 episodes: 992.308624\n",
      "Mean entropy of last action: 2.094650. Average score after 143 batches and 165 episodes: 995.450568\n",
      "Mean entropy of last action: 2.115478. Average score after 144 batches and 168 episodes: 997.802005\n",
      "Mean entropy of last action: 2.181345. Average score after 145 batches and 169 episodes: 1001.845435\n",
      "Mean entropy of last action: 2.202388. Average score after 146 batches and 169 episodes: 1001.845435\n",
      "Mean entropy of last action: 2.187810. Average score after 147 batches and 171 episodes: 995.770466\n",
      "Mean entropy of last action: 2.160249. Average score after 148 batches and 173 episodes: 995.142325\n",
      "Mean entropy of last action: 2.159074. Average score after 149 batches and 174 episodes: 997.167612\n",
      "Mean entropy of last action: 2.181387. Average score after 150 batches and 175 episodes: 993.442524\n",
      "Mean entropy of last action: 2.158236. Average score after 151 batches and 175 episodes: 993.442524\n",
      "Mean entropy of last action: 2.101595. Average score after 152 batches and 176 episodes: 993.663671\n",
      "Mean entropy of last action: 2.059888. Average score after 153 batches and 176 episodes: 993.663671\n",
      "Mean entropy of last action: 2.060531. Average score after 154 batches and 178 episodes: 999.471737\n",
      "Mean entropy of last action: 2.106151. Average score after 155 batches and 179 episodes: 1000.896713\n",
      "Mean entropy of last action: 2.122721. Average score after 156 batches and 180 episodes: 1001.376804\n",
      "Mean entropy of last action: 2.125618. Average score after 157 batches and 181 episodes: 1006.653244\n",
      "Mean entropy of last action: 2.116928. Average score after 158 batches and 181 episodes: 1006.653244\n",
      "Mean entropy of last action: 2.135010. Average score after 159 batches and 183 episodes: 1008.675590\n",
      "Mean entropy of last action: 2.161141. Average score after 160 batches and 183 episodes: 1008.675590\n",
      "Mean entropy of last action: 2.142232. Average score after 161 batches and 184 episodes: 1010.213068\n",
      "Mean entropy of last action: 2.121498. Average score after 162 batches and 186 episodes: 1009.308690\n",
      "Mean entropy of last action: 2.101421. Average score after 163 batches and 186 episodes: 1009.308690\n",
      "Mean entropy of last action: 2.125957. Average score after 164 batches and 187 episodes: 1015.148154\n",
      "Mean entropy of last action: 2.145695. Average score after 165 batches and 189 episodes: 1014.241244\n",
      "Mean entropy of last action: 2.154483. Average score after 166 batches and 189 episodes: 1014.241244\n",
      "Mean entropy of last action: 2.155956. Average score after 167 batches and 190 episodes: 1014.654327\n",
      "Mean entropy of last action: 2.138322. Average score after 168 batches and 191 episodes: 1015.654737\n",
      "Mean entropy of last action: 2.132351. Average score after 169 batches and 192 episodes: 1023.024905\n",
      "Mean entropy of last action: 2.121512. Average score after 170 batches and 192 episodes: 1023.024905\n",
      "Mean entropy of last action: 2.117713. Average score after 171 batches and 193 episodes: 1020.332842\n",
      "Mean entropy of last action: 2.131245. Average score after 172 batches and 195 episodes: 1017.481601\n",
      "Mean entropy of last action: 2.138990. Average score after 173 batches and 195 episodes: 1017.481601\n",
      "Mean entropy of last action: 2.146863. Average score after 174 batches and 196 episodes: 1022.259824\n",
      "Mean entropy of last action: 2.133680. Average score after 175 batches and 197 episodes: 1030.055882\n",
      "Mean entropy of last action: 2.139475. Average score after 176 batches and 199 episodes: 1038.675006\n",
      "Mean entropy of last action: 2.154025. Average score after 177 batches and 200 episodes: 1038.715568\n",
      "Mean entropy of last action: 2.157012. Average score after 178 batches and 200 episodes: 1038.715568\n",
      "Mean entropy of last action: 2.151225. Average score after 179 batches and 201 episodes: 1038.196941\n",
      "Mean entropy of last action: 2.128857. Average score after 180 batches and 201 episodes: 1038.196941\n",
      "Mean entropy of last action: 2.122923. Average score after 181 batches and 203 episodes: 1042.748618\n",
      "Mean entropy of last action: 2.164706. Average score after 182 batches and 205 episodes: 1042.785259\n",
      "Mean entropy of last action: 2.184112. Average score after 183 batches and 205 episodes: 1042.785259\n",
      "Mean entropy of last action: 2.179357. Average score after 184 batches and 205 episodes: 1042.785259\n",
      "Mean entropy of last action: 2.151326. Average score after 185 batches and 206 episodes: 1040.056548\n",
      "Mean entropy of last action: 2.131779. Average score after 186 batches and 207 episodes: 1047.578893\n",
      "Mean entropy of last action: 2.143584. Average score after 187 batches and 208 episodes: 1048.903230\n",
      "Mean entropy of last action: 2.137098. Average score after 188 batches and 209 episodes: 1053.945822\n",
      "Mean entropy of last action: 2.147640. Average score after 189 batches and 209 episodes: 1053.945822\n",
      "Mean entropy of last action: 2.139266. Average score after 190 batches and 211 episodes: 1055.966005\n",
      "Mean entropy of last action: 2.141476. Average score after 191 batches and 212 episodes: 1053.690980\n",
      "Mean entropy of last action: 2.145136. Average score after 192 batches and 213 episodes: 1060.773938\n",
      "Mean entropy of last action: 2.162372. Average score after 193 batches and 213 episodes: 1060.773938\n",
      "Mean entropy of last action: 2.166105. Average score after 194 batches and 214 episodes: 1065.794044\n",
      "Mean entropy of last action: 2.160076. Average score after 195 batches and 214 episodes: 1065.794044\n",
      "Mean entropy of last action: 2.175874. Average score after 196 batches and 215 episodes: 1065.382901\n",
      "Mean entropy of last action: 2.179763. Average score after 197 batches and 215 episodes: 1065.382901\n",
      "Mean entropy of last action: 2.183418. Average score after 198 batches and 217 episodes: 1071.635988\n",
      "Mean entropy of last action: 2.165371. Average score after 199 batches and 217 episodes: 1071.635988\n",
      "Mean entropy of last action: 2.152189. Average score after 200 batches and 218 episodes: 1077.634972\n",
      "Mean entropy of last action: 2.120426. Average score after 201 batches and 218 episodes: 1077.634972\n",
      "Mean entropy of last action: 2.095359. Average score after 202 batches and 220 episodes: 1084.688703\n",
      "Mean entropy of last action: 2.109192. Average score after 203 batches and 221 episodes: 1091.416417\n",
      "Mean entropy of last action: 2.133506. Average score after 204 batches and 222 episodes: 1092.295388\n",
      "Mean entropy of last action: 2.169684. Average score after 205 batches and 222 episodes: 1092.295388\n",
      "Mean entropy of last action: 2.146560. Average score after 206 batches and 223 episodes: 1093.512674\n",
      "Mean entropy of last action: 2.129839. Average score after 207 batches and 224 episodes: 1092.297968\n",
      "Mean entropy of last action: 2.134125. Average score after 208 batches and 226 episodes: 1088.776741\n",
      "Mean entropy of last action: 2.166672. Average score after 209 batches and 228 episodes: 1094.518219\n",
      "Mean entropy of last action: 2.180682. Average score after 210 batches and 228 episodes: 1094.518219\n",
      "Mean entropy of last action: 2.164662. Average score after 211 batches and 228 episodes: 1094.518219\n",
      "Mean entropy of last action: 2.135190. Average score after 212 batches and 230 episodes: 1098.310937\n",
      "Mean entropy of last action: 2.130038. Average score after 213 batches and 231 episodes: 1098.826881\n",
      "Mean entropy of last action: 2.128137. Average score after 214 batches and 231 episodes: 1098.826881\n",
      "Mean entropy of last action: 2.136321. Average score after 215 batches and 233 episodes: 1096.733492\n",
      "Mean entropy of last action: 2.123919. Average score after 216 batches and 233 episodes: 1096.733492\n",
      "Mean entropy of last action: 2.125706. Average score after 217 batches and 234 episodes: 1101.067751\n",
      "Mean entropy of last action: 2.137215. Average score after 218 batches and 234 episodes: 1101.067751\n",
      "Mean entropy of last action: 2.134234. Average score after 219 batches and 234 episodes: 1101.067751\n",
      "Mean entropy of last action: 2.135297. Average score after 220 batches and 235 episodes: 1107.083029\n",
      "Mean entropy of last action: 2.118742. Average score after 221 batches and 236 episodes: 1109.853820\n",
      "Mean entropy of last action: 2.142339. Average score after 222 batches and 236 episodes: 1109.853820\n",
      "Mean entropy of last action: 2.141801. Average score after 223 batches and 237 episodes: 1116.130957\n",
      "Mean entropy of last action: 2.142046. Average score after 224 batches and 237 episodes: 1116.130957\n",
      "Mean entropy of last action: 2.126237. Average score after 225 batches and 237 episodes: 1116.130957\n",
      "Mean entropy of last action: 2.126681. Average score after 226 batches and 239 episodes: 1128.335780\n",
      "Mean entropy of last action: 2.149284. Average score after 227 batches and 240 episodes: 1130.191776\n",
      "Mean entropy of last action: 2.183095. Average score after 228 batches and 241 episodes: 1135.665625\n",
      "Mean entropy of last action: 2.195248. Average score after 229 batches and 241 episodes: 1135.665625\n",
      "Mean entropy of last action: 2.203474. Average score after 230 batches and 241 episodes: 1135.665625\n",
      "Mean entropy of last action: 2.156086. Average score after 231 batches and 241 episodes: 1135.665625\n",
      "Mean entropy of last action: 2.122748. Average score after 232 batches and 243 episodes: 1144.708851\n",
      "Mean entropy of last action: 2.126621. Average score after 233 batches and 244 episodes: 1144.627260\n",
      "Mean entropy of last action: 2.144032. Average score after 234 batches and 244 episodes: 1144.627260\n",
      "Mean entropy of last action: 2.173765. Average score after 235 batches and 246 episodes: 1145.046092\n",
      "Mean entropy of last action: 2.149640. Average score after 236 batches and 246 episodes: 1145.046092\n",
      "Mean entropy of last action: 2.132170. Average score after 237 batches and 248 episodes: 1147.688690\n",
      "Mean entropy of last action: 2.141696. Average score after 238 batches and 248 episodes: 1147.688690\n",
      "Mean entropy of last action: 2.157240. Average score after 239 batches and 249 episodes: 1152.720932\n",
      "Mean entropy of last action: 2.181545. Average score after 240 batches and 249 episodes: 1152.720932\n",
      "Mean entropy of last action: 2.175263. Average score after 241 batches and 249 episodes: 1152.720932\n",
      "Mean entropy of last action: 2.143227. Average score after 242 batches and 250 episodes: 1151.846101\n",
      "Mean entropy of last action: 2.131031. Average score after 243 batches and 251 episodes: 1149.598912\n",
      "Mean entropy of last action: 2.159486. Average score after 244 batches and 254 episodes: 1162.142416\n",
      "Mean entropy of last action: 2.212976. Average score after 245 batches and 254 episodes: 1162.142416\n",
      "Mean entropy of last action: 2.207001. Average score after 246 batches and 255 episodes: 1168.085476\n",
      "Mean entropy of last action: 2.162696. Average score after 247 batches and 256 episodes: 1165.938147\n",
      "Mean entropy of last action: 2.124209. Average score after 248 batches and 256 episodes: 1165.938147\n",
      "Mean entropy of last action: 2.116100. Average score after 249 batches and 257 episodes: 1163.882704\n",
      "Mean entropy of last action: 2.133230. Average score after 250 batches and 257 episodes: 1163.882704\n",
      "Mean entropy of last action: 2.131731. Average score after 251 batches and 258 episodes: 1163.604867\n",
      "Mean entropy of last action: 2.129500. Average score after 252 batches and 258 episodes: 1163.604867\n",
      "Mean entropy of last action: 2.128412. Average score after 253 batches and 261 episodes: 1168.669695\n",
      "Mean entropy of last action: 2.155013. Average score after 254 batches and 261 episodes: 1168.669695\n",
      "Mean entropy of last action: 2.175250. Average score after 255 batches and 262 episodes: 1173.711066\n",
      "Mean entropy of last action: 2.188118. Average score after 256 batches and 262 episodes: 1173.711066\n",
      "Mean entropy of last action: 2.178748. Average score after 257 batches and 262 episodes: 1173.711066\n",
      "Mean entropy of last action: 2.169725. Average score after 258 batches and 264 episodes: 1178.162674\n",
      "Mean entropy of last action: 2.188023. Average score after 259 batches and 264 episodes: 1178.162674\n",
      "Mean entropy of last action: 2.207375. Average score after 260 batches and 265 episodes: 1183.597503\n",
      "Mean entropy of last action: 2.228353. Average score after 261 batches and 265 episodes: 1183.597503\n",
      "Mean entropy of last action: 2.201385. Average score after 262 batches and 265 episodes: 1183.597503\n",
      "Mean entropy of last action: 2.169574. Average score after 263 batches and 265 episodes: 1183.597503\n",
      "Mean entropy of last action: 2.129823. Average score after 264 batches and 266 episodes: 1182.813531\n",
      "Mean entropy of last action: 2.165008. Average score after 265 batches and 270 episodes: 1195.715872\n",
      "Mean entropy of last action: 2.194201. Average score after 266 batches and 270 episodes: 1195.715872\n",
      "Mean entropy of last action: 2.216986. Average score after 267 batches and 270 episodes: 1195.715872\n",
      "Mean entropy of last action: 2.193388. Average score after 268 batches and 270 episodes: 1195.715872\n",
      "Mean entropy of last action: 2.182172. Average score after 269 batches and 273 episodes: 1199.017020\n",
      "Mean entropy of last action: 2.200471. Average score after 270 batches and 274 episodes: 1199.188710\n",
      "Mean entropy of last action: 2.226218. Average score after 271 batches and 275 episodes: 1198.895059\n",
      "Mean entropy of last action: 2.236580. Average score after 272 batches and 276 episodes: 1196.696109\n",
      "Mean entropy of last action: 2.206645. Average score after 273 batches and 276 episodes: 1196.696109\n",
      "Mean entropy of last action: 2.173518. Average score after 274 batches and 276 episodes: 1196.696109\n",
      "Mean entropy of last action: 2.151935. Average score after 275 batches and 277 episodes: 1202.188910\n",
      "Mean entropy of last action: 2.151043. Average score after 276 batches and 278 episodes: 1201.777135\n",
      "Mean entropy of last action: 2.158577. Average score after 277 batches and 278 episodes: 1201.777135\n",
      "Mean entropy of last action: 2.164355. Average score after 278 batches and 278 episodes: 1201.777135\n",
      "Mean entropy of last action: 2.156048. Average score after 279 batches and 278 episodes: 1201.777135\n",
      "Mean entropy of last action: 2.127784. Average score after 280 batches and 281 episodes: 1210.375513\n",
      "Mean entropy of last action: 2.148372. Average score after 281 batches and 283 episodes: 1217.924195\n",
      "Mean entropy of last action: 2.183730. Average score after 282 batches and 283 episodes: 1217.924195\n",
      "Mean entropy of last action: 2.218919. Average score after 283 batches and 283 episodes: 1217.924195\n",
      "Mean entropy of last action: 2.198429. Average score after 284 batches and 283 episodes: 1217.924195\n",
      "Mean entropy of last action: 2.155653. Average score after 285 batches and 284 episodes: 1217.028404\n",
      "Mean entropy of last action: 2.143437. Average score after 286 batches and 285 episodes: 1216.723854\n",
      "Mean entropy of last action: 2.157286. Average score after 287 batches and 285 episodes: 1216.723854\n",
      "Mean entropy of last action: 2.177272. Average score after 288 batches and 286 episodes: 1221.531553\n",
      "Mean entropy of last action: 2.174798. Average score after 289 batches and 286 episodes: 1221.531553\n",
      "Mean entropy of last action: 2.149439. Average score after 290 batches and 286 episodes: 1221.531553\n",
      "Mean entropy of last action: 2.132363. Average score after 291 batches and 286 episodes: 1221.531553\n",
      "Mean entropy of last action: 2.127664. Average score after 292 batches and 287 episodes: 1226.358237\n",
      "Mean entropy of last action: 2.168557. Average score after 293 batches and 289 episodes: 1235.614972\n",
      "Mean entropy of last action: 2.176253. Average score after 294 batches and 290 episodes: 1232.720840\n",
      "Mean entropy of last action: 2.180077. Average score after 295 batches and 291 episodes: 1235.427782\n",
      "Mean entropy of last action: 2.169011. Average score after 296 batches and 292 episodes: 1239.857170\n",
      "Mean entropy of last action: 2.151975. Average score after 297 batches and 293 episodes: 1238.731418\n",
      "Mean entropy of last action: 2.156461. Average score after 298 batches and 295 episodes: 1240.430038\n",
      "Mean entropy of last action: 2.174108. Average score after 299 batches and 296 episodes: 1239.876229\n",
      "Mean entropy of last action: 2.222320. Average score after 300 batches and 298 episodes: 1236.149654\n",
      "Mean entropy of last action: 2.240465. Average score after 301 batches and 298 episodes: 1236.149654\n",
      "Mean entropy of last action: 2.240658. Average score after 302 batches and 300 episodes: 1236.290066\n",
      "Mean entropy of last action: 2.226197. Average score after 303 batches and 300 episodes: 1236.290066\n",
      "Mean entropy of last action: 2.210134. Average score after 304 batches and 301 episodes: 1236.171242\n",
      "Mean entropy of last action: 2.187540. Average score after 305 batches and 301 episodes: 1236.171242\n",
      "Mean entropy of last action: 2.182612. Average score after 306 batches and 302 episodes: 1235.962606\n",
      "Mean entropy of last action: 2.186818. Average score after 307 batches and 303 episodes: 1234.808891\n",
      "Mean entropy of last action: 2.206867. Average score after 308 batches and 303 episodes: 1234.808891\n",
      "Mean entropy of last action: 2.203582. Average score after 309 batches and 304 episodes: 1237.169706\n",
      "Mean entropy of last action: 2.196050. Average score after 310 batches and 305 episodes: 1236.453366\n",
      "Mean entropy of last action: 2.192597. Average score after 311 batches and 307 episodes: 1242.114708\n",
      "Mean entropy of last action: 2.203729. Average score after 312 batches and 308 episodes: 1239.338347\n",
      "Mean entropy of last action: 2.220016. Average score after 313 batches and 310 episodes: 1240.566276\n"
     ]
    }
   ],
   "source": [
    "batch_states = []\n",
    "batch_actions = []\n",
    "batch_advantages = []\n",
    "batch_last_states = []\n",
    "batch_final_state = []\n",
    "batch_count = 0\n",
    "baseline = 0\n",
    "\n",
    "for first_state,advantage,last_state,first_action,final_state in exp:\n",
    "    batch_states.append(first_state)\n",
    "    batch_advantages.append(advantage)\n",
    "    batch_final_state.append(final_state)\n",
    "    batch_last_states.append(last_state)\n",
    "    batch_actions.append(first_action)\n",
    "    \n",
    "    \n",
    "    if len(batch_states) == BATCH_SIZE:\n",
    "        batch_states = np.array(batch_states).reshape(NUM_ENVS*BATCH_SIZE,subEnvs.observation_space.shape[1])\n",
    "        batch_actions = np.array(batch_actions).reshape(NUM_ENVS*BATCH_SIZE)\n",
    "        batch_advantages = np.array(batch_advantages).reshape(NUM_ENVS*BATCH_SIZE)\n",
    "        batch_final_state = np.array(batch_final_state).reshape(NUM_ENVS*BATCH_SIZE)\n",
    "        batch_last_states = np.array(batch_last_states).reshape(NUM_ENVS*BATCH_SIZE,subEnvs.observation_space.shape[1])\n",
    "        \n",
    "        if policy == None: \n",
    "            policy = NetPG(68,11).to(device)\n",
    "            exp.update(policy,critic)\n",
    "            optimizer = optim.Adam(policy.parameters(), lr=LEARNING_RATE)\n",
    "            \n",
    "        delta = torch.FloatTensor(batch_advantages).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits_v = policy( torch.FloatTensor(batch_states).to(device))\n",
    "        log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "        log_prob_actions_v = delta * log_prob_v[:,batch_actions]\n",
    "        loss_policy_v = -log_prob_actions_v.mean()\n",
    "        \n",
    "        \n",
    "        prob_v = F.softmax(logits_v, dim=1)\n",
    "        entropy_v = -(prob_v * log_prob_v).sum(dim=1).mean()\n",
    "        entropy_loss_v = -ENTROPY_BETA * entropy_v\n",
    "\n",
    "        loss_v = loss_policy_v + entropy_loss_v\n",
    "\n",
    "        loss_v.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "                        \n",
    "        batch_states = []\n",
    "        batch_actions = []\n",
    "        batch_advantages = []\n",
    "        batch_last_states = []\n",
    "        batch_final_state = []\n",
    "\n",
    "        gc.collect()\n",
    "        batch_count+=1\n",
    "        \n",
    "        \n",
    "        episode_score,entropy,nepisodes = exp.getStats()\n",
    "        writer.add_scalar(\"Episode Score\", episode_score, nepisodes)\n",
    "        writer.add_scalar(\"Entropy\", entropy, nepisodes)\n",
    "        #writer.add_scalar(\"Last Gradient variance\", np.var(g), nepisodes)\n",
    "        \n",
    "        if batch_count % 1 == 0:\n",
    "            print(\"Mean entropy of last action: %2f. Average score after %d batches and %d episodes: %2f\" % (exp.getStats()[1],batch_count,nepisodes,exp.getStats()[0]))        \n",
    "        #if batch_count % 10 == 0:\n",
    "            #policy.save_model(\"policy_last_roundabout_ann.model\")\n",
    "            #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy, \"ANN-Roundabout_NOCRITIC_Nenv_%d_batch_length_%d_nsteps_%d_eta_%2f.model\" % (NUM_ENVS,BATCH_SIZE,nsteps,LEARNING_RATE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
